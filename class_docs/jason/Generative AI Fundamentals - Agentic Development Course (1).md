& How Agents WorkUnderstanding the technology before using itAgentic Development CourseWhat We'll Cover Today1What is Generative AI?GPT, tokens, training2Fun ExercisesCreative generation3From Autocomplete to AgentsReAct framework4LabHands-on experimentationPart 1What is Generative AI?Understanding AI TrainingTwo short videos explaining how AI learns:ğŸ¬How AI Image Generators Workyoutube.com/watch?v=R9OHn5ZF4UoğŸ¬How Large Language Models Workyoutube.com/watch?v=wvWpdrfoEv0A Brief History of Generative AIYearMilestone1957Perceptron - First trainable neuralnetwork1961ELIZA - First chatbot (earlygenerative AI)1979Neocognitron - First deep learningneural network1989Backpropagation - Deep learningbecomes practical1997LSTM - Long short-term memory forspeech recognitionSource: dataversity.net/articles/a-brief-history-of-generative-aiThe Modern Era of Generative AIYearBreakthrough2014GANs - Generate realistic images,video, audio2017Transformers - "Attention Is All YouNeed" paper2022ChatGPT - LLMs go mainstream2023+Agentic AI - Systems that plan andtake actionsğŸ’¡Most of what we call "AI" today happened in the last 10 yearsGPT = ?GenerativeCreates new contentPre-trainedLearned from massive data before you use itTransformerThe architecture that made this possible (2017)The Core Insight"World's Best Autocomplete"The Core Insight"World's Best Autocomplete"The Core Insight"World's Best Autocomplete"At its heart: predicting "what word comes next"Trained on billions of text examplesDoes prediction SO well it appears intelligentNot "thinking" â€” pattern matching atunprecedented scaleHow Generative AI Works (Interactive)ğŸ”—ig.ft.com/generative-aiHow Training Works1Feed billions of text examplesBooks, websites, code, conversations2Learn to predict next tokenGiven previous tokens3Scale upMore data + more parameters = emergent capabilities4Fine-tune for conversationRLHF (Reinforcement Learning from Human Feedback)Key Concept: TokensTokens are subword pieces (~4 characters average)TextTokens"hello"1 token"uncomfortable"["un", "comfort", "able"] = 3tokensCodeOften more tokens per line thanEnglishWhy it matters: You pay per token, limits are intokensKey Concept: Context WindowThe model's "working memory"System promptConversation historyYour current messageDocuments/codeClaude~200K tokensGPT-4~128K tokensGemini 1.5~1M tokensKey Concept: TemperatureğŸ”§Lower for code/factsğŸ¨Higher for creative writingKey Concept: HallucinationsWhy AI makes things upKey Concept: HallucinationsModel generates plausible next tokensPlausible â‰  TrueConfident prediction â‰  Factual informationâš Always verify important outputsThe Jagged Frontier of AIThe Jagged Frontier - Key InsightsğŸ†Superhuman at unexpected tasksMedical diagnosis, complex math, sophisticated codeğŸ¤”Struggles with "simple" tasksVisual puzzles, counting, physical reasoningğŸ¯Jaggedness doesn't match intuitionPasses bar exam, fails at basic visual tasksğŸ¤Creates collaboration opportunitiesHumans fill AI gaps, AI amplifies human strengthsSource: Ethan Mollick, "The Shape of AI"The Equation of Agentic WorkThe Equation - Key Factors1Human Baseline TimeHow long would this take YOU to do?2Probability of SuccessHow likely is AI to succeed? (Remember the jagged frontier)3AI Process TimeAgents run in background while you work on other thingsğŸ’¡Management skills become your superpower with AIagentsSource: Ethan Mollick, "Management as AISuperpower"Management as AI SuperpowerPart 2Fun Generative ExercisesThese aren't just games â€” they reveal how the modelworksExercise: The Dinosaur Rewrite ğŸ¦–Exercise: The Dinosaur Rewrite ğŸ¦–Take this news article: [paste any recent news]Rewrite it so that a dinosaur is somehowcentrally involved in the incident.Keep the same journalistic tone and structure.Exercise: The Tone Dial ğŸšOriginal email:"The project deadline was missed again. This isunacceptable. We need to discuss this."1. Furiousâ†’2. Frustratedâ†’3. Neutralâ†’4. Understandingâ†’5. GraciousExercise: Format Juggling ğŸ”„Input:"John Smith is a 34-year-old software engineerfrom Seattle. He earns $150,000 at TechCorp..."JSONYAMLBullet pointsSQL INSERTHaiku ğŸ‹Movie trailer ğŸ¬Same information, endless formats!Exercise: The Accordion ğŸª—Start with: "The server crashed."â†— ExpandIncident report (1 paragraph)â†— ExpandPost-mortem (3 paragraphs)â†˜ CompressTweet (280 chars)â†˜ CompressSingle emojiPart 3From Autocomplete to AgentsThe conceptual leap that changes everythingThe LimitationLLMs can only produce textThe Limitationâœ—Can't browse the webâœ—Can't run codeâœ—Can't read filesâœ—Can't call APIs"All talk, no action"The Solution: ToolsGive the LLM ability to request actionsUser"What's the weather in Seattle?"â†“LLM thinks"I need weather data..."â†“LLM outputs{"tool": "get_weather", "location": "Seattle"}â†“System executes{"temp": 52, "condition": "rainy"}â†“LLM responds"It's 52Â°F and rainy in Seattle."The ReAct FrameworkReasoning + ActingThe ReAct FrameworkReasoning + ActingğŸ¤”THINKReason about taskâ†’âš¡ACTTool callâ†’ğŸ‘OBSERVESee resultWhy ReAct Works1Explicit reasoningPrevents rushing to wrong actions2Observation stepAllows course correction3Loop continuesUntil task is complete4More reliableThan single-shot generationActivity: Be the LLMActivity: Be the LLMğŸ§ 1 student = The LLMReads prompt, generatesresponseğŸ”§1 student = Tool ExecutorRuns tools, returnsresultsLLM outputs either:TOOL: [name], INPUT: [value]orANSWER:[response]Prompt â†’ LLM (Round 1)SYSTEM:You are a helpful assistant. Follow this loop:1. THINK out loud about what you need to do2. ACT by calling a tool if needed3. OBSERVE the result4. REPEAT until you can answerTools: calculator(expression), web_search(query)USER:What is the square root of 65536?LLM Response (Round 1)[LLM reasoning]TOOL: calculatorINPUT: sqrt(65536)Tool Executor: run the calculationTool Result256Prompt â†’ LLM (Round 2)SYSTEM:You are a helpful assistant. Follow this loop:1. THINK out loud about what you need to do2. ACT by calling a tool if needed3. OBSERVE the result4. REPEAT until you can answerTools: calculator(expression), web_search(query)USER:What is the square root of 65536?ASSISTANT:[LLM's reasoning from Round 1]TOOL: calculatorINPUT: sqrt(65536)LLM Response (Round 2)[LLM reasoning]ANSWER: The square root of 65536 is 256.What Makes an "Agent"ğŸ§ LLM"the brain"+ğŸ”§Tools"the hands"+ğŸ”„Reasoning Loop"the process"=ğŸ¤–Autonomous AgentCommon Agent ToolsTool TypeExamplesğŸ“ File systemRead, write, search filesğŸŒ WebFetch pages, searchâš™ Code executionRun scripts, testsğŸ”Œ APIsExternal servicesğŸ­ BrowserPlaywright for web interactionWhy Agents Matter for DevelopmentBeforeAfterAI suggests codeAI READS your codeYou run testsAI RUNS your testsYou research librariesAI RESEARCHES for youYou fix issuesAI FIXES and verifiesTransforms AI from "assistant" to "autonomousdeveloper"Why Agents Matter for DevelopmentPart 4Lab TimePut these concepts into practiceLab Exercises1Token Exploration5 minUse a tokenizer to explore how text splitsplatform.openai.com/tokenizer â†’2Creative Generation10 minRewrite your project description in 3 stylesOr: Technical concept as children's story / rap /news3Agent Thinking5-10 minWrite out ReAct steps for researching a libraryKey Takeaways1LLMs = sophisticated autocompletePredicting tokens, not "thinking"2Tokens â‰  wordsUnderstanding tokens helps efficiency3Agents = LLM + tools + loopWhat makes AI useful for dev4ReAct: Think â†’ Act â†’ Observe â†’ RepeatThe foundational agent pattern5Always verifyHallucinations happenBefore Next Timeâ˜Read: "Management as AI Superpower" - oneusefulthing.orgâ˜Think: What would you want an AI agent to research for yourproject?Next time: Ideation & Planning with AI â€” brainstorming, marketresearch, PRDsQuestions?ResourcesğŸ”¤OpenAI Tokenizerplatform.openai.com/tokenizerğŸ“šClaude Documentationdocs.anthropic.comğŸ“„ReAct Paperarxiv.org/abs/2210.03629ğŸ“–Management as AI Superpoweroneusefulthing.orgSee you next time!Next: Ideation & Planning with AIAgentic Development CourseSpeaker notes